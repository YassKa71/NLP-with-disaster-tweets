{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-11T14:48:43.539818Z","iopub.execute_input":"2023-11-11T14:48:43.540194Z","iopub.status.idle":"2023-11-11T14:48:43.906855Z","shell.execute_reply.started":"2023-11-11T14:48:43.540166Z","shell.execute_reply":"2023-11-11T14:48:43.905954Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-with-disaster-tweets-cleaning-data/test_data_cleaning.csv\n/kaggle/input/nlp-with-disaster-tweets-cleaning-data/train_data_cleaning.csv\n/kaggle/input/nlp-with-disaster-tweets-cleaning-data/test_data_cleaning2.csv\n/kaggle/input/nlp-with-disaster-tweets-cleaning-data/train_data_cleaning2.csv\n/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\nimport re\nfrom sklearn import feature_extraction, linear_model, model_selection, metrics","metadata":{"execution":{"iopub.status.busy":"2023-11-11T14:48:44.281744Z","iopub.execute_input":"2023-11-11T14:48:44.282267Z","iopub.status.idle":"2023-11-11T14:48:57.605075Z","shell.execute_reply.started":"2023-11-11T14:48:44.282237Z","shell.execute_reply":"2023-11-11T14:48:57.603865Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# reading train and test files\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-11T14:48:57.607474Z","iopub.execute_input":"2023-11-11T14:48:57.610042Z","iopub.status.idle":"2023-11-11T14:48:57.683751Z","shell.execute_reply.started":"2023-11-11T14:48:57.610010Z","shell.execute_reply":"2023-11-11T14:48:57.682607Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# displaying the 5 first columns of train_df\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T14:48:57.685069Z","iopub.execute_input":"2023-11-11T14:48:57.685370Z","iopub.status.idle":"2023-11-11T14:48:57.706856Z","shell.execute_reply.started":"2023-11-11T14:48:57.685345Z","shell.execute_reply":"2023-11-11T14:48:57.705638Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# split the data into train, test and validation sets\nX_train, X_temp, y_train, y_temp = model_selection.train_test_split(train_df[\"text\"], train_df[\"target\"], test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = model_selection.train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T14:48:57.709869Z","iopub.execute_input":"2023-11-11T14:48:57.710247Z","iopub.status.idle":"2023-11-11T14:48:57.722306Z","shell.execute_reply.started":"2023-11-11T14:48:57.710218Z","shell.execute_reply":"2023-11-11T14:48:57.721166Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!pip install tweet-preprocessor","metadata":{"execution":{"iopub.status.busy":"2023-11-11T14:48:57.723686Z","iopub.execute_input":"2023-11-11T14:48:57.724527Z","iopub.status.idle":"2023-11-11T14:49:11.165118Z","shell.execute_reply.started":"2023-11-11T14:48:57.724480Z","shell.execute_reply":"2023-11-11T14:49:11.164031Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting tweet-preprocessor\n  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\nInstalling collected packages: tweet-preprocessor\nSuccessfully installed tweet-preprocessor-0.6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install gensim","metadata":{"execution":{"iopub.status.busy":"2023-11-11T14:49:11.166486Z","iopub.execute_input":"2023-11-11T14:49:11.166806Z","iopub.status.idle":"2023-11-11T14:49:23.115441Z","shell.execute_reply.started":"2023-11-11T14:49:11.166777Z","shell.execute_reply":"2023-11-11T14:49:23.114144Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.2)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.23.5)\nRequirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.11.2)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (6.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import preprocessor as p\nfrom gensim.parsing.preprocessing import remove_stopwords\ndef preprocess_tweet(text):\n    text = remove_stopwords(text)\n    text = text.lower()\n    text = re.sub('[^\\w\\s]','',text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-11-11T14:49:23.117124Z","iopub.execute_input":"2023-11-11T14:49:23.117446Z","iopub.status.idle":"2023-11-11T14:49:46.033333Z","shell.execute_reply.started":"2023-11-11T14:49:23.117419Z","shell.execute_reply":"2023-11-11T14:49:46.032284Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# text data preprocessing\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\ndef preprocessing(X):\n    max_length = max(len(text) for text in X)\n    input_ids = []\n    attention_masks = []\n    for text in X:\n        text = preprocess_tweet(text)\n        encoded_dict = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors=\"pt\"\n        )\n        input_ids.append(encoded_dict[\"input_ids\"])\n        attention_masks.append(encoded_dict[\"attention_mask\"])\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    return input_ids, attention_masks\n\ntrain_input_ids, train_attention_masks = preprocessing(X_train)\nvalidation_input_ids, validation_attention_masks = preprocessing(X_val)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T14:49:46.034570Z","iopub.execute_input":"2023-11-11T14:49:46.034880Z","iopub.status.idle":"2023-11-11T14:49:52.683412Z","shell.execute_reply.started":"2023-11-11T14:49:46.034854Z","shell.execute_reply":"2023-11-11T14:49:52.682567Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dc4cf84186a4cbf9480696d2bdc3268"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6361d8186284517953820f08949b5bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a08d8f3ed20048c1a0defef56842b6a0"}},"metadata":{}}]},{"cell_type":"code","source":"train_labels = torch.tensor(y_train.values)\nvalidation_labels = torch.tensor(y_val.values)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T14:49:52.685979Z","iopub.execute_input":"2023-11-11T14:49:52.686285Z","iopub.status.idle":"2023-11-11T14:49:52.695149Z","shell.execute_reply.started":"2023-11-11T14:49:52.686258Z","shell.execute_reply":"2023-11-11T14:49:52.694204Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# create a data loader\ndef create_data_loader(input_ids, attention_masks, labels):\n    dataset = TensorDataset(input_ids, attention_masks, labels)\n    batch_size = 32\n    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    return train_loader\n\ntrain_loader = create_data_loader(train_input_ids, train_attention_masks, train_labels)\nvalidation_loader = create_data_loader(validation_input_ids, validation_attention_masks, validation_labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T14:49:52.696819Z","iopub.execute_input":"2023-11-11T14:49:52.697161Z","iopub.status.idle":"2023-11-11T14:49:52.703921Z","shell.execute_reply.started":"2023-11-11T14:49:52.697129Z","shell.execute_reply":"2023-11-11T14:49:52.702939Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# define model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\nmodel.classifier = nn.Sequential(\n    nn.Linear(model.config.hidden_size, 256),\n    nn.ReLU(),\n    nn.Dropout(0.1),\n    nn.Linear(256, 100),\n    nn.ReLU(),\n    nn.Dropout(0.1),\n    nn.Linear(100, 50),\n    nn.ReLU(),\n    nn.Linear(50, 2),\n)\n\n# freeze parameters\nfor param in model.bert.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-11-11T14:49:52.705097Z","iopub.execute_input":"2023-11-11T14:49:52.705426Z","iopub.status.idle":"2023-11-11T14:49:56.345299Z","shell.execute_reply.started":"2023-11-11T14:49:52.705396Z","shell.execute_reply":"2023-11-11T14:49:56.344538Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f71729cb83e844209d9ad3d454c63a37"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# defining loss function and optimizer\nfrom transformers import get_linear_schedule_with_warmup\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.0001)\ntotal_steps = len(train_loader) * 6\n# Create a learning rate scheduler\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T14:49:56.346357Z","iopub.execute_input":"2023-11-11T14:49:56.346641Z","iopub.status.idle":"2023-11-11T14:49:56.354054Z","shell.execute_reply.started":"2023-11-11T14:49:56.346616Z","shell.execute_reply":"2023-11-11T14:49:56.353107Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# fine tune the model\nnum_epochs = 6\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ntrain_losses, valid_losses = [], []\nvalid_loss_min = np.Inf\n\nfor epoch in range(num_epochs):\n    train_loss, valid_loss = 0 , 0\n    model.train()\n    for batch in train_loader:\n        batch = tuple(t.to(device) for t in batch)\n        inputs = {\n            \"input_ids\": batch[0],\n            \"attention_mask\": batch[1],\n            \"labels\": batch[2]\n        }\n        optimizer.zero_grad()\n        outputs = model(**inputs)\n        loss = criterion(outputs.logits, inputs[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * inputs[\"labels\"].size(0)\n    \n    model.eval()\n    for batch in validation_loader:\n        batch = tuple(t.to(device) for t in batch)\n        inputs = {\n            \"input_ids\": batch[0],\n            \"attention_mask\": batch[1],\n            \"labels\": batch[2]\n        }\n        with torch.no_grad():\n              output = model(**inputs)\n        loss = criterion(output.logits,inputs[\"labels\"])\n        valid_loss += loss.item() * inputs[\"labels\"].size(0)\n\n    train_loss /= len(train_loader.sampler)\n    valid_loss /= len(validation_loader.sampler)\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n\n    print('epoch: {} \\ttraining Loss: {:.6f} \\tvalidation Loss: {:.6f}'.format(epoch+1, train_loss, valid_loss))\n\n    if valid_loss <= valid_loss_min:\n        print('validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'model.pt')\n        valid_loss_min = valid_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-11T15:14:22.307992Z","iopub.execute_input":"2023-11-11T15:14:22.309134Z","iopub.status.idle":"2023-11-11T15:20:29.078892Z","shell.execute_reply.started":"2023-11-11T15:14:22.309091Z","shell.execute_reply":"2023-11-11T15:20:29.077893Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"epoch: 1 \ttraining Loss: 0.565733 \tvalidation Loss: 0.539589\nvalidation loss decreased (inf --> 0.539589).  Saving model ...\nepoch: 2 \ttraining Loss: 0.569161 \tvalidation Loss: 0.539589\nvalidation loss decreased (0.539589 --> 0.539589).  Saving model ...\nepoch: 3 \ttraining Loss: 0.561218 \tvalidation Loss: 0.539589\nepoch: 4 \ttraining Loss: 0.563292 \tvalidation Loss: 0.539589\nepoch: 5 \ttraining Loss: 0.565949 \tvalidation Loss: 0.539589\nepoch: 6 \ttraining Loss: 0.562666 \tvalidation Loss: 0.539589\n","output_type":"stream"}]},{"cell_type":"code","source":"# testing the model and evaluating it using F1 score\ntest_input_ids, test_attention_masks = preprocessing(X_test)\ntest_labels = torch.tensor(y_test.values)\ntest_loader = create_data_loader(test_input_ids, test_attention_masks, test_labels)\n\ntrue_labels = []\npredicted_labels = []\n\nmodel.load_state_dict(torch.load('model.pt', map_location=device))\n\nmodel.eval()\nfor batch in test_loader:\n    batch = tuple(t.to(device) for t in batch)\n    inputs = {\n        \"input_ids\": batch[0],\n        \"attention_mask\": batch[1],\n        \"labels\": batch[2]\n    }\n    with torch.no_grad():\n        output = model(**inputs)\n    _, predicted = torch.max(output.logits, 1)\n    true_labels.extend(inputs[\"labels\"].cpu().numpy())\n    predicted_labels.extend(predicted.cpu().numpy())\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T14:56:02.029511Z","iopub.execute_input":"2023-11-11T14:56:02.029896Z","iopub.status.idle":"2023-11-11T14:56:13.883433Z","shell.execute_reply.started":"2023-11-11T14:56:02.029868Z","shell.execute_reply":"2023-11-11T14:56:13.882097Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(len(true_labels))\nprint(predicted_labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T21:36:41.664926Z","iopub.status.idle":"2023-11-08T21:36:41.665458Z","shell.execute_reply.started":"2023-11-08T21:36:41.665183Z","shell.execute_reply":"2023-11-08T21:36:41.665205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1 = metrics.f1_score(true_labels, predicted_labels)\nprint(f\"F1 Score: {f1}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-08T22:34:27.212819Z","iopub.execute_input":"2023-11-08T22:34:27.213116Z","iopub.status.idle":"2023-11-08T22:34:27.223023Z","shell.execute_reply.started":"2023-11-08T22:34:27.213091Z","shell.execute_reply":"2023-11-08T22:34:27.222140Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"F1 Score: 0.7364016736401673\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}