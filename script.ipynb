{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"},{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":2606439,"sourceType":"datasetVersion","datasetId":771947}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-19T19:12:47.518509Z","iopub.execute_input":"2023-11-19T19:12:47.518772Z","iopub.status.idle":"2023-11-19T19:12:47.911621Z","shell.execute_reply.started":"2023-11-19T19:12:47.518747Z","shell.execute_reply":"2023-11-19T19:12:47.910785Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-with-disaster-tweets-cleaning-data/test_data_cleaning.csv\n/kaggle/input/nlp-with-disaster-tweets-cleaning-data/train_data_cleaning.csv\n/kaggle/input/nlp-with-disaster-tweets-cleaning-data/test_data_cleaning2.csv\n/kaggle/input/nlp-with-disaster-tweets-cleaning-data/train_data_cleaning2.csv\n/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# NLP with Disaster Tweets #","metadata":{}},{"cell_type":"markdown","source":"## I- RigidClassifier","metadata":{}},{"cell_type":"markdown","source":"We will start by assuming that words present in each tweet are a great indicator of disasters. Therefore, in the code below we will use Count Vectorizer to count the number of times each token appears in each tweet, and then we will use a linear model (scikit-learn Rigid Classifier) to classify whether there is a real disaster or not. In fact, we will suppose that there is a linear separation between the two classes.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import feature_extraction, linear_model, model_selection, metrics","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:12:47.913666Z","iopub.execute_input":"2023-11-19T19:12:47.914157Z","iopub.status.idle":"2023-11-19T19:12:49.840960Z","shell.execute_reply.started":"2023-11-19T19:12:47.914122Z","shell.execute_reply":"2023-11-19T19:12:49.839997Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# reading train and test files\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:12:49.842325Z","iopub.execute_input":"2023-11-19T19:12:49.842888Z","iopub.status.idle":"2023-11-19T19:12:49.937846Z","shell.execute_reply.started":"2023-11-19T19:12:49.842857Z","shell.execute_reply":"2023-11-19T19:12:49.936958Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# displaying the 5 first columns of train_df\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:12:49.940639Z","iopub.execute_input":"2023-11-19T19:12:49.941046Z","iopub.status.idle":"2023-11-19T19:12:49.968302Z","shell.execute_reply.started":"2023-11-19T19:12:49.941002Z","shell.execute_reply":"2023-11-19T19:12:49.967404Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# using CountVectorizer: tokenize all the texts and create a sparse matrix where each row represents a document,\n# and each column represents a unique token. The cell values indicate how many times each token appears in each document.\n\ncountvectorizer = feature_extraction.text.CountVectorizer()\ntrain_countvectorizer_matrix = countvectorizer.fit_transform(train_df[\"text\"])\nprint(f\"Resulting countvectorizer matrix shape: {train_countvectorizer_matrix.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:12:49.969413Z","iopub.execute_input":"2023-11-19T19:12:49.970124Z","iopub.status.idle":"2023-11-19T19:12:50.335350Z","shell.execute_reply.started":"2023-11-19T19:12:49.970096Z","shell.execute_reply":"2023-11-19T19:12:50.334318Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Resulting countvectorizer matrix shape: (7613, 21637)\n","output_type":"stream"}]},{"cell_type":"code","source":"# using scikit learn RidgeClassifier as a model\nclassifier = linear_model.RidgeClassifier()\n\n# apply cross validation on the model and display the score\nscore = model_selection.cross_val_score(estimator= classifier, X= train_countvectorizer_matrix, y=train_df[\"target\"], cv = 5, scoring = \"f1\")\nscore","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:12:50.336727Z","iopub.execute_input":"2023-11-19T19:12:50.337094Z","iopub.status.idle":"2023-11-19T19:12:52.430773Z","shell.execute_reply.started":"2023-11-19T19:12:50.337067Z","shell.execute_reply":"2023-11-19T19:12:52.429597Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"array([0.6025641 , 0.50168919, 0.56985004, 0.50781969, 0.67275495])"},"metadata":{}}]},{"cell_type":"markdown","source":"The score obtained with Ridge classifier is around 0.55.\nTo improve this score we will next take into account the context in each tweet instead of only counting the number of times certain tokens appear.\n\nIn order to do so, we will fine-tune Bert pretrained models.","metadata":{}},{"cell_type":"markdown","source":"## II- Fine-tuning Bert pretrained model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\nimport re\nfrom sklearn import feature_extraction, linear_model, model_selection, metrics","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:12:52.432092Z","iopub.execute_input":"2023-11-19T19:12:52.432680Z","iopub.status.idle":"2023-11-19T19:13:15.483155Z","shell.execute_reply.started":"2023-11-19T19:12:52.432644Z","shell.execute_reply":"2023-11-19T19:13:15.482334Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"## fill NA values in keyword and location columns\ntrain_df.keyword = train_df.keyword.fillna('')\ntrain_df.location = train_df.location.fillna('')\n\n## Combine location and keyword with the main text \ntrain_df['full_text'] = train_df.location + ' ' + train_df.keyword + ' ' + train_df.text","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:13:15.484221Z","iopub.execute_input":"2023-11-19T19:13:15.484484Z","iopub.status.idle":"2023-11-19T19:13:15.501140Z","shell.execute_reply.started":"2023-11-19T19:13:15.484460Z","shell.execute_reply":"2023-11-19T19:13:15.500150Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into train, validation and test","metadata":{}},{"cell_type":"code","source":"# split the data into train, test and validation sets\nX_train, X_temp, y_train, y_temp = model_selection.train_test_split(train_df[\"full_text\"], train_df[\"target\"], test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = model_selection.train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:13:15.502751Z","iopub.execute_input":"2023-11-19T19:13:15.503053Z","iopub.status.idle":"2023-11-19T19:13:15.518818Z","shell.execute_reply.started":"2023-11-19T19:13:15.503028Z","shell.execute_reply":"2023-11-19T19:13:15.517987Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install tweet-preprocessor","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:13:15.522073Z","iopub.execute_input":"2023-11-19T19:13:15.522328Z","iopub.status.idle":"2023-11-19T19:13:29.659226Z","shell.execute_reply.started":"2023-11-19T19:13:15.522306Z","shell.execute_reply":"2023-11-19T19:13:29.658213Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Collecting tweet-preprocessor\n  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\nInstalling collected packages: tweet-preprocessor\nSuccessfully installed tweet-preprocessor-0.6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install gensim","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:13:29.660710Z","iopub.execute_input":"2023-11-19T19:13:29.661099Z","iopub.status.idle":"2023-11-19T19:13:41.691318Z","shell.execute_reply.started":"2023-11-19T19:13:29.661058Z","shell.execute_reply":"2023-11-19T19:13:41.690221Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.2)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.23.5)\nRequirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.11.2)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (6.3.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Data Preprocessing","metadata":{}},{"cell_type":"code","source":"import preprocessor as p\ndef preprocess_tweet(text):\n    text = text.lower()\n    text = re.sub('[^\\w\\s]','',text)\n    re.sub(r'http\\S+', '', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:13:41.692907Z","iopub.execute_input":"2023-11-19T19:13:41.693231Z","iopub.status.idle":"2023-11-19T19:13:41.721416Z","shell.execute_reply.started":"2023-11-19T19:13:41.693199Z","shell.execute_reply":"2023-11-19T19:13:41.720718Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# text data preprocessing\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\ndef preprocessing(X):\n    max_length = max(len(text) for text in X)\n    input_ids = []\n    attention_masks = []\n    for text in X:\n        text = preprocess_tweet(text)\n        encoded_dict = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors=\"pt\"\n        )\n        input_ids.append(encoded_dict[\"input_ids\"])\n        attention_masks.append(encoded_dict[\"attention_mask\"])\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    return input_ids, attention_masks\n\ntrain_input_ids, train_attention_masks = preprocessing(X_train)\nvalidation_input_ids, validation_attention_masks = preprocessing(X_val)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:13:41.722574Z","iopub.execute_input":"2023-11-19T19:13:41.723355Z","iopub.status.idle":"2023-11-19T19:13:50.371136Z","shell.execute_reply.started":"2023-11-19T19:13:41.723319Z","shell.execute_reply":"2023-11-19T19:13:50.370140Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33c26aa9c2334e3293febc10341d2c75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fba0737da36e492f899f018e28bb85d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c85a066b0d0143dca6161b350760f484"}},"metadata":{}}]},{"cell_type":"code","source":"train_labels = torch.tensor(y_train.values)\nvalidation_labels = torch.tensor(y_val.values)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:13:50.372368Z","iopub.execute_input":"2023-11-19T19:13:50.372682Z","iopub.status.idle":"2023-11-19T19:13:50.387514Z","shell.execute_reply.started":"2023-11-19T19:13:50.372639Z","shell.execute_reply":"2023-11-19T19:13:50.386603Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Data Loaders for training and validation","metadata":{}},{"cell_type":"code","source":"# create a data loader\ndef create_data_loader(input_ids, attention_masks, labels):\n    dataset = TensorDataset(input_ids, attention_masks, labels)\n    batch_size = 32\n    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    return train_loader\n\ntrain_loader = create_data_loader(train_input_ids, train_attention_masks, train_labels)\nvalidation_loader = create_data_loader(validation_input_ids, validation_attention_masks, validation_labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:13:50.388659Z","iopub.execute_input":"2023-11-19T19:13:50.388973Z","iopub.status.idle":"2023-11-19T19:13:50.395738Z","shell.execute_reply.started":"2023-11-19T19:13:50.388926Z","shell.execute_reply":"2023-11-19T19:13:50.394895Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Defining the model, optimizer, and loss function ","metadata":{}},{"cell_type":"code","source":"# define model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\nmodel.classifier = nn.Sequential(\n    nn.Linear(model.config.hidden_size, 256),\n    nn.ReLU(),\n    nn.Dropout(0.1),\n    nn.Linear(256, 2),\n)\n\n# freeze parameters\nfor param in model.bert.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:13:50.396758Z","iopub.execute_input":"2023-11-19T19:13:50.397068Z","iopub.status.idle":"2023-11-19T19:13:53.929463Z","shell.execute_reply.started":"2023-11-19T19:13:50.397034Z","shell.execute_reply":"2023-11-19T19:13:53.928697Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e986c111abf247088087df4c457b2ed3"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# defining loss function and optimizer\nfrom transformers import get_linear_schedule_with_warmup\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\ntotal_steps = len(train_loader) * 30\n# Create a learning rate scheduler\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:13:53.930548Z","iopub.execute_input":"2023-11-19T19:13:53.930827Z","iopub.status.idle":"2023-11-19T19:13:53.938953Z","shell.execute_reply.started":"2023-11-19T19:13:53.930802Z","shell.execute_reply":"2023-11-19T19:13:53.938194Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Fine tuning the model","metadata":{}},{"cell_type":"code","source":"# fine tune the model\nnum_epochs = 30\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ntrain_losses, valid_losses = [], []\nvalid_loss_min = np.Inf\n\nfor epoch in range(num_epochs):\n    train_loss, valid_loss = 0 , 0\n    model.train()\n    for batch in train_loader:\n        batch = tuple(t.to(device) for t in batch)\n        inputs = {\n            \"input_ids\": batch[0],\n            \"attention_mask\": batch[1],\n            \"labels\": batch[2]\n        }\n        optimizer.zero_grad()\n        outputs = model(**inputs)\n        loss = criterion(outputs.logits, inputs[\"labels\"])\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        train_loss += loss.item() * inputs[\"labels\"].size(0)\n    \n    model.eval()\n    for batch in validation_loader:\n        batch = tuple(t.to(device) for t in batch)\n        inputs = {\n            \"input_ids\": batch[0],\n            \"attention_mask\": batch[1],\n            \"labels\": batch[2]\n        }\n        with torch.no_grad():\n              output = model(**inputs)\n        loss = criterion(output.logits,inputs[\"labels\"])\n        valid_loss += loss.item() * inputs[\"labels\"].size(0)\n\n    train_loss /= len(train_loader.sampler)\n    valid_loss /= len(validation_loader.sampler)\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n\n    print('epoch: {} \\ttraining Loss: {:.6f} \\tvalidation Loss: {:.6f}'.format(epoch+1, train_loss, valid_loss))\n\n    if valid_loss <= valid_loss_min:\n        print('validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'model.pt')\n        valid_loss_min = valid_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:13:53.940125Z","iopub.execute_input":"2023-11-19T19:13:53.940448Z","iopub.status.idle":"2023-11-19T19:50:08.367161Z","shell.execute_reply.started":"2023-11-19T19:13:53.940415Z","shell.execute_reply":"2023-11-19T19:50:08.366154Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"epoch: 1 \ttraining Loss: 0.621891 \tvalidation Loss: 0.651734\nvalidation loss decreased (inf --> 0.651734).  Saving model ...\nepoch: 2 \ttraining Loss: 0.573217 \tvalidation Loss: 0.547575\nvalidation loss decreased (0.651734 --> 0.547575).  Saving model ...\nepoch: 3 \ttraining Loss: 0.550110 \tvalidation Loss: 0.578565\nepoch: 4 \ttraining Loss: 0.549973 \tvalidation Loss: 0.526134\nvalidation loss decreased (0.547575 --> 0.526134).  Saving model ...\nepoch: 5 \ttraining Loss: 0.538517 \tvalidation Loss: 0.525436\nvalidation loss decreased (0.526134 --> 0.525436).  Saving model ...\nepoch: 6 \ttraining Loss: 0.530203 \tvalidation Loss: 0.494742\nvalidation loss decreased (0.525436 --> 0.494742).  Saving model ...\nepoch: 7 \ttraining Loss: 0.527712 \tvalidation Loss: 0.502406\nepoch: 8 \ttraining Loss: 0.529782 \tvalidation Loss: 0.495369\nepoch: 9 \ttraining Loss: 0.521471 \tvalidation Loss: 0.496375\nepoch: 10 \ttraining Loss: 0.523993 \tvalidation Loss: 0.717052\nepoch: 11 \ttraining Loss: 0.525722 \tvalidation Loss: 0.488532\nvalidation loss decreased (0.494742 --> 0.488532).  Saving model ...\nepoch: 12 \ttraining Loss: 0.515629 \tvalidation Loss: 0.489334\nepoch: 13 \ttraining Loss: 0.514491 \tvalidation Loss: 0.529184\nepoch: 14 \ttraining Loss: 0.518954 \tvalidation Loss: 0.523195\nepoch: 15 \ttraining Loss: 0.514052 \tvalidation Loss: 0.489268\nepoch: 16 \ttraining Loss: 0.505057 \tvalidation Loss: 0.481197\nvalidation loss decreased (0.488532 --> 0.481197).  Saving model ...\nepoch: 17 \ttraining Loss: 0.511024 \tvalidation Loss: 0.488584\nepoch: 18 \ttraining Loss: 0.506123 \tvalidation Loss: 0.479293\nvalidation loss decreased (0.481197 --> 0.479293).  Saving model ...\nepoch: 19 \ttraining Loss: 0.504036 \tvalidation Loss: 0.501120\nepoch: 20 \ttraining Loss: 0.497859 \tvalidation Loss: 0.511950\nepoch: 21 \ttraining Loss: 0.506638 \tvalidation Loss: 0.478756\nvalidation loss decreased (0.479293 --> 0.478756).  Saving model ...\nepoch: 22 \ttraining Loss: 0.499254 \tvalidation Loss: 0.481905\nepoch: 23 \ttraining Loss: 0.494436 \tvalidation Loss: 0.474672\nvalidation loss decreased (0.478756 --> 0.474672).  Saving model ...\nepoch: 24 \ttraining Loss: 0.503440 \tvalidation Loss: 0.473925\nvalidation loss decreased (0.474672 --> 0.473925).  Saving model ...\nepoch: 25 \ttraining Loss: 0.496292 \tvalidation Loss: 0.472454\nvalidation loss decreased (0.473925 --> 0.472454).  Saving model ...\nepoch: 26 \ttraining Loss: 0.495894 \tvalidation Loss: 0.479251\nepoch: 27 \ttraining Loss: 0.501160 \tvalidation Loss: 0.479483\nepoch: 28 \ttraining Loss: 0.497038 \tvalidation Loss: 0.475847\nepoch: 29 \ttraining Loss: 0.495128 \tvalidation Loss: 0.477465\nepoch: 30 \ttraining Loss: 0.496449 \tvalidation Loss: 0.479639\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Testing the model on unseen data and evaluation","metadata":{}},{"cell_type":"code","source":"# testing the model and evaluating it using F1 score\ntest_input_ids, test_attention_masks = preprocessing(X_test)\ntest_labels = torch.tensor(y_test.values)\ntest_loader = create_data_loader(test_input_ids, test_attention_masks, test_labels)\n\ntrue_labels = []\npredicted_labels = []\n\nmodel.load_state_dict(torch.load('model.pt', map_location=device))\n\nmodel.eval()\nfor batch in test_loader:\n    batch = tuple(t.to(device) for t in batch)\n    inputs = {\n        \"input_ids\": batch[0],\n        \"attention_mask\": batch[1],\n        \"labels\": batch[2]\n    }\n    with torch.no_grad():\n        output = model(**inputs)\n    _, predicted = torch.max(output.logits, 1)\n    true_labels.extend(inputs[\"labels\"].cpu().numpy())\n    predicted_labels.extend(predicted.cpu().numpy())\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:50:08.368596Z","iopub.execute_input":"2023-11-19T19:50:08.369282Z","iopub.status.idle":"2023-11-19T19:50:23.089236Z","shell.execute_reply.started":"2023-11-19T19:50:08.369246Z","shell.execute_reply":"2023-11-19T19:50:23.088249Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"f1 = metrics.f1_score(true_labels, predicted_labels)\nprint(f\"F1 Score: {f1}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:50:23.090605Z","iopub.execute_input":"2023-11-19T19:50:23.091293Z","iopub.status.idle":"2023-11-19T19:50:23.101906Z","shell.execute_reply.started":"2023-11-19T19:50:23.091258Z","shell.execute_reply":"2023-11-19T19:50:23.100908Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"F1 Score: 0.7500000000000001\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see an improvement in the obtained F1 score. Lets see how we can improve it even more by displaying examples of bad predictions.","metadata":{}},{"cell_type":"markdown","source":"### Interpretation of the score","metadata":{}},{"cell_type":"code","source":"# displaying examples of bad predictions\nbad_pred_index = [i for i in range(len(predicted_labels)) if predicted_labels[i] != true_labels[i]]\ntest_index_list = X_test.index.tolist()\ntest_set_array = train_df.iloc[test_index_list]\ntest_set_array[\"full_text\"] = [preprocess_tweet(text) for text in test_set_array[\"full_text\"]] \ntest_set_array[\"predicted labels\"] = predicted_labels\ntest_set_array[\"true labels\"] = true_labels\ntest_set_bad_pred= test_set_array.iloc[bad_pred_index]\npd.set_option('display.max_colwidth', None)\ntest_set_bad_pred[1:10]\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T20:10:40.077984Z","iopub.execute_input":"2023-11-19T20:10:40.078853Z","iopub.status.idle":"2023-11-19T20:10:40.121059Z","shell.execute_reply.started":"2023-11-19T20:10:40.078822Z","shell.execute_reply":"2023-11-19T20:10:40.120134Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3993678858.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test_set_array[\"full_text\"] = [preprocess_tweet(text) for text in test_set_array[\"full_text\"]]\n/tmp/ipykernel_31/3993678858.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test_set_array[\"predicted labels\"] = predicted_labels\n/tmp/ipykernel_31/3993678858.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test_set_array[\"true labels\"] = true_labels\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"         id              keyword                  location  \\\n7561  10810              wrecked                         6   \n5386   7686                panic              Milwaukee WI   \n3541   5063               famine             New York, USA   \n5498   7847          quarantined                             \n640     929                blaze                California   \n3021   4336         dust%20storm               CA via Brum   \n1215   1753  buildings%20burning  somewhere over a rainbow   \n4771   6789            lightning                   Reddit    \n3098   4448         electrocuted                             \n\n                                                                                                                                                  text  \\\n7561                                                                                                                     @Tunes_WGG lol. U got wrecked   \n5386          Someone asked me about a monkey fist about 2 feet long with a panic snap like the one pictured to be used as a... http://t.co/Yi9BBbx3FE   \n3541                                                           'Food crematoria' provoke outrage amid crisis famine memories... http://t.co/fABVlvN5MS   \n5498                      Top link: Reddit's new content policy goes into effect many horrible subreddits banned or quarantined http://t.co/u9ao3A4oGC   \n640                                                                                                                            @Kaotix_Blaze craving u   \n3021        Wall of noise is one thing - but a wall of dust? Moving at 60MPH? http://t.co/9NwAJLi9cr How to not get blown away! http://t.co/j4NI4N0yFZ   \n1215  @DoctorFluxx @StefanEJones @spinnellii @themermacorn  No burning buildings and rob during a riot. That's embarrassing &amp; ruining this nation.   \n4771                                                                         Lightning strike in the distance via /r/pics http://t.co/iDmhSwewQw #pics   \n3098                                                             When I was cooking earlier I got electrocuted some crucial ?????? now I'm psychic lol   \n\n      target  \\\n7561       0   \n5386       0   \n3541       1   \n5498       0   \n640        0   \n3021       1   \n1215       1   \n4771       1   \n3098       0   \n\n                                                                                                                                                                                full_text  \\\n7561                                                                                                                                                6 wrecked tunes_wgg lol u got wrecked   \n5386                                  milwaukee wi panic someone asked me about a monkey fist about 2 feet long with a panic snap like the one pictured to be used as a httptcoyi9bbbx3fe   \n3541                                                                                    new york usa famine food crematoria provoke outrage amid crisis famine memories httptcofabvlvn5ms   \n5498                                                    quarantined top link reddits new content policy goes into effect many horrible subreddits banned or quarantined httptcou9ao3a4ogc   \n640                                                                                                                                               california blaze kaotix_blaze craving u   \n3021                                 ca via brum dust20storm wall of noise is one thing  but a wall of dust moving at 60mph httptco9nwajli9cr how to not get blown away httptcoj4ni4n0yfz   \n1215  somewhere over a rainbow buildings20burning doctorfluxx stefanejones spinnellii themermacorn  no burning buildings and rob during a riot thats embarrassing amp ruining this nation   \n4771                                                                                                  reddit  lightning lightning strike in the distance via rpics httptcoidmhswewqw pics   \n3098                                                                                          electrocuted when i was cooking earlier i got electrocuted some crucial  now im psychic lol   \n\n      predicted labels  true labels  \n7561                 1            0  \n5386                 0            1  \n3541                 0            1  \n5498                 0            1  \n640                  0            1  \n3021                 1            0  \n1215                 0            1  \n4771                 0            1  \n3098                 1            0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>full_text</th>\n      <th>predicted labels</th>\n      <th>true labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7561</th>\n      <td>10810</td>\n      <td>wrecked</td>\n      <td>6</td>\n      <td>@Tunes_WGG lol. U got wrecked</td>\n      <td>0</td>\n      <td>6 wrecked tunes_wgg lol u got wrecked</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5386</th>\n      <td>7686</td>\n      <td>panic</td>\n      <td>Milwaukee WI</td>\n      <td>Someone asked me about a monkey fist about 2 feet long with a panic snap like the one pictured to be used as a... http://t.co/Yi9BBbx3FE</td>\n      <td>0</td>\n      <td>milwaukee wi panic someone asked me about a monkey fist about 2 feet long with a panic snap like the one pictured to be used as a httptcoyi9bbbx3fe</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3541</th>\n      <td>5063</td>\n      <td>famine</td>\n      <td>New York, USA</td>\n      <td>'Food crematoria' provoke outrage amid crisis famine memories... http://t.co/fABVlvN5MS</td>\n      <td>1</td>\n      <td>new york usa famine food crematoria provoke outrage amid crisis famine memories httptcofabvlvn5ms</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5498</th>\n      <td>7847</td>\n      <td>quarantined</td>\n      <td></td>\n      <td>Top link: Reddit's new content policy goes into effect many horrible subreddits banned or quarantined http://t.co/u9ao3A4oGC</td>\n      <td>0</td>\n      <td>quarantined top link reddits new content policy goes into effect many horrible subreddits banned or quarantined httptcou9ao3a4ogc</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>640</th>\n      <td>929</td>\n      <td>blaze</td>\n      <td>California</td>\n      <td>@Kaotix_Blaze craving u</td>\n      <td>0</td>\n      <td>california blaze kaotix_blaze craving u</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3021</th>\n      <td>4336</td>\n      <td>dust%20storm</td>\n      <td>CA via Brum</td>\n      <td>Wall of noise is one thing - but a wall of dust? Moving at 60MPH? http://t.co/9NwAJLi9cr How to not get blown away! http://t.co/j4NI4N0yFZ</td>\n      <td>1</td>\n      <td>ca via brum dust20storm wall of noise is one thing  but a wall of dust moving at 60mph httptco9nwajli9cr how to not get blown away httptcoj4ni4n0yfz</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1215</th>\n      <td>1753</td>\n      <td>buildings%20burning</td>\n      <td>somewhere over a rainbow</td>\n      <td>@DoctorFluxx @StefanEJones @spinnellii @themermacorn  No burning buildings and rob during a riot. That's embarrassing &amp;amp; ruining this nation.</td>\n      <td>1</td>\n      <td>somewhere over a rainbow buildings20burning doctorfluxx stefanejones spinnellii themermacorn  no burning buildings and rob during a riot thats embarrassing amp ruining this nation</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4771</th>\n      <td>6789</td>\n      <td>lightning</td>\n      <td>Reddit</td>\n      <td>Lightning strike in the distance via /r/pics http://t.co/iDmhSwewQw #pics</td>\n      <td>1</td>\n      <td>reddit  lightning lightning strike in the distance via rpics httptcoidmhswewqw pics</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3098</th>\n      <td>4448</td>\n      <td>electrocuted</td>\n      <td></td>\n      <td>When I was cooking earlier I got electrocuted some crucial ?????? now I'm psychic lol</td>\n      <td>0</td>\n      <td>electrocuted when i was cooking earlier i got electrocuted some crucial  now im psychic lol</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# displaying examples of good predictions\ngood_pred_index = [i for i in range(len(predicted_labels)) if predicted_labels[i] == true_labels[i]]\ntest_set_good_pred= test_set_array.iloc[good_pred_index]\npd.set_option('display.max_colwidth', None)\ntest_set_good_pred[1:10]","metadata":{"execution":{"iopub.status.busy":"2023-11-19T20:10:49.292070Z","iopub.execute_input":"2023-11-19T20:10:49.292829Z","iopub.status.idle":"2023-11-19T20:10:49.308207Z","shell.execute_reply.started":"2023-11-19T20:10:49.292796Z","shell.execute_reply":"2023-11-19T20:10:49.307232Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"         id          keyword         location  \\\n2662   3824         detonate        Worldwide   \n5378   7675            panic    Elsewhere, NZ   \n4740   6742             lava              USA   \n4013   5699           floods                    \n7278  10418        whirlwind   London, Sydney   \n3912   5563            flood    United States   \n2484   3567         desolate                    \n7486  10708            wreck                    \n4864   6926  mass%20murderer  Hemel Hempstead   \n\n                                                                                                                                             text  \\\n2662                                                   52.214904 5.139055 Nuke please. Target Hilversum please detonate 800 meters below surface.   \n5378                                                       Lose bus card.\\nPanic.\\nKind bus driver.\\nReplace bus card.\\nFind bus card.\\nHeaddesk.   \n4740  Check This Deal : http://t.co/uOoYgBb6aZ Sivan Health and Fitness Basalt Lava Hot Stone Massage Kit with 36 PieceÛ_ http://t.co/JJxcnwBp15   \n4013  Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE   \n7278                                                            Two hours to get to a client meeting. Whirlwind of emotions with this #tubestrike   \n3912                                                        JKL cancels Flash Flood Warning for Bell Harlan Knox [KY]  http://t.co/4rY6zhcPOQ #WX   \n2484                                                        If the Taken movies took place in India 2 (Vine by @JusReign) https://t.co/hxM8C8e33D   \n7486                                                                                                            I'm an emotional wreck right now.   \n4864  If your friends really were your friends they'd support you regardless of your decisions.\\n\\nUnless you become a mass-murderer or something   \n\n      target  \\\n2662       1   \n5378       0   \n4740       0   \n4013       0   \n7278       1   \n3912       1   \n2484       0   \n7486       0   \n4864       0   \n\n                                                                                                                                                                    full_text  \\\n2662                                                                worldwide detonate 52214904 5139055 nuke please target hilversum please detonate 800 meters below surface   \n5378                                                                      elsewhere nz panic lose bus card\\npanic\\nkind bus driver\\nreplace bus card\\nfind bus card\\nheaddesk   \n4740                                 usa lava check this deal  httptcouooygbb6az sivan health and fitness basalt lava hot stone massage kit with 36 pieceû_ httptcojjxcnwbp15   \n4013                           floods who is bringing the tornadoes and floods who is bringing the climate change god is after america he is plaguing her\\n \\nfarrakhan quote   \n7278                                                                  london sydney whirlwind two hours to get to a client meeting whirlwind of emotions with this tubestrike   \n3912                                                                        united states flood jkl cancels flash flood warning for bell harlan knox ky  httptco4ry6zhcpoq wx   \n2484                                                                                   desolate if the taken movies took place in india 2 vine by jusreign httpstcohxm8c8e33d   \n7486                                                                                                                                    wreck im an emotional wreck right now   \n4864  hemel hempstead mass20murderer if your friends really were your friends theyd support you regardless of your decisions\\n\\nunless you become a massmurderer or something   \n\n      predicted labels  true labels  \n2662                 0            0  \n5378                 1            1  \n4740                 0            0  \n4013                 0            0  \n7278                 0            0  \n3912                 0            0  \n2484                 0            0  \n7486                 0            0  \n4864                 1            1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>full_text</th>\n      <th>predicted labels</th>\n      <th>true labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2662</th>\n      <td>3824</td>\n      <td>detonate</td>\n      <td>Worldwide</td>\n      <td>52.214904 5.139055 Nuke please. Target Hilversum please detonate 800 meters below surface.</td>\n      <td>1</td>\n      <td>worldwide detonate 52214904 5139055 nuke please target hilversum please detonate 800 meters below surface</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5378</th>\n      <td>7675</td>\n      <td>panic</td>\n      <td>Elsewhere, NZ</td>\n      <td>Lose bus card.\\nPanic.\\nKind bus driver.\\nReplace bus card.\\nFind bus card.\\nHeaddesk.</td>\n      <td>0</td>\n      <td>elsewhere nz panic lose bus card\\npanic\\nkind bus driver\\nreplace bus card\\nfind bus card\\nheaddesk</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4740</th>\n      <td>6742</td>\n      <td>lava</td>\n      <td>USA</td>\n      <td>Check This Deal : http://t.co/uOoYgBb6aZ Sivan Health and Fitness Basalt Lava Hot Stone Massage Kit with 36 PieceÛ_ http://t.co/JJxcnwBp15</td>\n      <td>0</td>\n      <td>usa lava check this deal  httptcouooygbb6az sivan health and fitness basalt lava hot stone massage kit with 36 pieceû_ httptcojjxcnwbp15</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4013</th>\n      <td>5699</td>\n      <td>floods</td>\n      <td></td>\n      <td>Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE</td>\n      <td>0</td>\n      <td>floods who is bringing the tornadoes and floods who is bringing the climate change god is after america he is plaguing her\\n \\nfarrakhan quote</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7278</th>\n      <td>10418</td>\n      <td>whirlwind</td>\n      <td>London, Sydney</td>\n      <td>Two hours to get to a client meeting. Whirlwind of emotions with this #tubestrike</td>\n      <td>1</td>\n      <td>london sydney whirlwind two hours to get to a client meeting whirlwind of emotions with this tubestrike</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3912</th>\n      <td>5563</td>\n      <td>flood</td>\n      <td>United States</td>\n      <td>JKL cancels Flash Flood Warning for Bell Harlan Knox [KY]  http://t.co/4rY6zhcPOQ #WX</td>\n      <td>1</td>\n      <td>united states flood jkl cancels flash flood warning for bell harlan knox ky  httptco4ry6zhcpoq wx</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2484</th>\n      <td>3567</td>\n      <td>desolate</td>\n      <td></td>\n      <td>If the Taken movies took place in India 2 (Vine by @JusReign) https://t.co/hxM8C8e33D</td>\n      <td>0</td>\n      <td>desolate if the taken movies took place in india 2 vine by jusreign httpstcohxm8c8e33d</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7486</th>\n      <td>10708</td>\n      <td>wreck</td>\n      <td></td>\n      <td>I'm an emotional wreck right now.</td>\n      <td>0</td>\n      <td>wreck im an emotional wreck right now</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4864</th>\n      <td>6926</td>\n      <td>mass%20murderer</td>\n      <td>Hemel Hempstead</td>\n      <td>If your friends really were your friends they'd support you regardless of your decisions.\\n\\nUnless you become a mass-murderer or something</td>\n      <td>0</td>\n      <td>hemel hempstead mass20murderer if your friends really were your friends theyd support you regardless of your decisions\\n\\nunless you become a massmurderer or something</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see through the examples, that the preprocessing of the dataset can be improved by : not removing the punctuation, removing the links to websites, and finding a better way to combine keyword and location to the text that will not change the meaning of the text.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}